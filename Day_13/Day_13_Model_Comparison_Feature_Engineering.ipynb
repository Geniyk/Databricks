{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef010c3f-1703-4a52-b9d5-a9a2d1fd59a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Train 3 Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a80dceb-d7f0-44a2-8833-cd8855e9da87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-01-21 08:41:19.277\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\", grpc_status:13, created_time:\\\"2026-01-21T08:41:19.277004163+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_analyze\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1811\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2026-01-21 08:41:19.277\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\", grpc_status:13, created_time:\\\"2026-01-21T08:41:19.277004163+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_analyze\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1811\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2026-01-21 08:41:19.277\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\", grpc_status:13, created_time:\\\"2026-01-21T08:41:19.277004163+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_analyze\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1811\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n2026/01/21 08:41:19 WARNING mlflow.spark: Failed to infer the model signature from the input example. Reason: IllegalArgumentException('requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.'). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`.\n2026/01/21 08:41:26 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/01/21 08:41:30 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-07530eca-bcfd-48b6-bf02-e9/tmpcb6x67tp/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/01/21 08:41:30 WARNING mlflow.models.model: Model logged without a signature. Signatures are required for Databricks UC model registry as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.22.0/model/signatures.html#how-to-set-signatures-on-models for instructions on setting signature on models.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression | RMSE = 19570667.46\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-01-21 08:41:55.704\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2026-01-21T08:41:55.703602162+00:00\\\", grpc_status:13, grpc_message:\\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_analyze\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1811\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2026-01-21 08:41:55.704\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2026-01-21T08:41:55.703602162+00:00\\\", grpc_status:13, grpc_message:\\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_analyze\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1811\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2026-01-21 08:41:55.704\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {created_time:\\\"2026-01-21T08:41:55.703602162+00:00\\\", grpc_status:13, grpc_message:\\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_analyze\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1811\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\nERROR:pyspark.sql.connect.logging:GRPC Error received\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 1811, in _analyze\n    resp = self._stub.AnalyzePlan(req, metadata=self.metadata())\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", line 277, in __call__\n    response, ignored_call = self._with_call(\n                             ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", line 332, in _with_call\n    return call.result(), call\n           ^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", line 440, in result\n    raise self\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", line 315, in continuation\n    response, call = self._thunk(new_method).with_call(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", line 343, in with_call\n    return self._with_call(\n           ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", line 332, in _with_call\n    return call.result(), call\n           ^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", line 440, in result\n    raise self\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", line 315, in continuation\n    response, call = self._thunk(new_method).with_call(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", line 1198, in with_call\n    return _end_unary_response_blocking(state, call, True, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", line 1006, in _end_unary_response_blocking\n    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ngrpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2026-01-21T08:41:55.703602162+00:00\", grpc_status:13, grpc_message:\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\"}\"\n>\n2026/01/21 08:41:55 WARNING mlflow.spark: Failed to infer the model signature from the input example. Reason: IllegalArgumentException('requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.'). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`.\n2026/01/21 08:42:01 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/01/21 08:42:03 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-07530eca-bcfd-48b6-bf02-e9/tmp01yie2es/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/01/21 08:42:03 WARNING mlflow.models.model: Model logged without a signature. Signatures are required for Databricks UC model registry as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.22.0/model/signatures.html#how-to-set-signatures-on-models for instructions on setting signature on models.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree | RMSE = 79878445.21\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-01-21 08:42:16.749\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\", grpc_status:13, created_time:\\\"2026-01-21T08:42:16.748863706+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_analyze\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1811\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2026-01-21 08:42:16.749\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\", grpc_status:13, created_time:\\\"2026-01-21T08:42:16.748863706+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_analyze\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1811\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\n{\"ts\": \"2026-01-21 08:42:16.749\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\\\", grpc_status:13, created_time:\\\"2026-01-21T08:42:16.748863706+00:00\\\"}\\\"\\n>\", \"stacktrace\": [{\"class\": null, \"method\": \"_analyze\", \"file\": \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", \"line\": \"1811\"}, {\"class\": null, \"method\": \"__call__\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"277\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"343\"}, {\"class\": null, \"method\": \"_with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"332\"}, {\"class\": null, \"method\": \"result\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"440\"}, {\"class\": null, \"method\": \"continuation\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", \"line\": \"315\"}, {\"class\": null, \"method\": \"with_call\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1198\"}, {\"class\": null, \"method\": \"_end_unary_response_blocking\", \"file\": \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", \"line\": \"1006\"}]}}\nERROR:pyspark.sql.connect.logging:GRPC Error received\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py\", line 1811, in _analyze\n    resp = self._stub.AnalyzePlan(req, metadata=self.metadata())\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", line 277, in __call__\n    response, ignored_call = self._with_call(\n                             ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", line 332, in _with_call\n    return call.result(), call\n           ^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", line 440, in result\n    raise self\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", line 315, in continuation\n    response, call = self._thunk(new_method).with_call(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", line 343, in with_call\n    return self._with_call(\n           ^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", line 332, in _with_call\n    return call.result(), call\n           ^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", line 440, in result\n    raise self\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_interceptor.py\", line 315, in continuation\n    response, call = self._thunk(new_method).with_call(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", line 1198, in with_call\n    return _end_unary_response_blocking(state, call, True, None)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.12/site-packages/grpc/_channel.py\", line 1006, in _end_unary_response_blocking\n    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ngrpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INTERNAL\n\tdetails = \"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.\", grpc_status:13, created_time:\"2026-01-21T08:42:16.748863706+00:00\"}\"\n>\n2026/01/21 08:42:16 WARNING mlflow.spark: Failed to infer the model signature from the input example. Reason: IllegalArgumentException('requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.ArrayType:array<double>.'). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`.\n2026/01/21 08:42:23 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/01/21 08:42:26 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-07530eca-bcfd-48b6-bf02-e9/tmpt8q9czq_/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/01/21 08:42:26 WARNING mlflow.models.model: Model logged without a signature. Signatures are required for Databricks UC model registry as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.22.0/model/signatures.html#how-to-set-signatures-on-models for instructions on setting signature on models.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest | RMSE = 68778849.78\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import (\n",
    "    LinearRegression,\n",
    "    DecisionTreeRegressor,\n",
    "    RandomForestRegressor\n",
    ")\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Unity Catalog requirement\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/workspace/ecommerce/mlflow_tmp\"\n",
    "\n",
    "# Set MLflow Experiment\n",
    "mlflow.set_experiment(\"/day13-mlflow-model-comparison\")\n",
    "\n",
    "# Load data\n",
    "data = (\n",
    "    spark.table(\"ecommerce.silver.daily_sales\")\n",
    "    .select(\"total_events\", \"total_revenue\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "train_df, test_df = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Feature Engineering\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"total_events\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "train_vec = assembler.transform(train_df)\n",
    "test_vec = assembler.transform(test_df)\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"total_revenue\"\n",
    "    ),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"total_revenue\",\n",
    "        maxDepth=5\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestRegressor(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=\"total_revenue\",\n",
    "        numTrees=50\n",
    "    )\n",
    "}\n",
    "\n",
    "# Evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"total_revenue\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "# Train & log\n",
    "for name, model in models.items():\n",
    "\n",
    "    with mlflow.start_run(run_name=name):\n",
    "\n",
    "        mlflow.log_param(\"model_type\", name)\n",
    "        mlflow.log_param(\"features\", \"total_events\")\n",
    "\n",
    "        fitted_model = model.fit(train_vec)\n",
    "        predictions = fitted_model.transform(test_vec)\n",
    "\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "        #  FIX: Convert DenseVector â†’ list\n",
    "        sample_rows = (\n",
    "            train_vec\n",
    "            .select(\"features\")\n",
    "            .limit(5)\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        input_example = pd.DataFrame({\n",
    "            \"features\": [row[\"features\"].toArray().tolist() for row in sample_rows]\n",
    "        })\n",
    "\n",
    "        mlflow.spark.log_model(\n",
    "            spark_model=fitted_model,\n",
    "            artifact_path=\"model\",\n",
    "            input_example=input_example\n",
    "        )\n",
    "\n",
    "        print(f\"{name} | RMSE = {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee25cae2-00e2-4694-a7d5-686e8833a817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags.mlflow.runName</th>\n",
       "      <th>metrics.rmse</th>\n",
       "      <th>params.model_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>6.877885e+07</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>7.987845e+07</td>\n",
       "      <td>DecisionTree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>1.957067e+07</td>\n",
       "      <td>LinearRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>1.957067e+07</td>\n",
       "      <td>LinearRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>1.957067e+07</td>\n",
       "      <td>LinearRegression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tags.mlflow.runName  metrics.rmse params.model_type\n",
       "0        RandomForest  6.877885e+07      RandomForest\n",
       "1        DecisionTree  7.987845e+07      DecisionTree\n",
       "2    LinearRegression  1.957067e+07  LinearRegression\n",
       "3    LinearRegression  1.957067e+07  LinearRegression\n",
       "4    LinearRegression  1.957067e+07  LinearRegression"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "runs = mlflow.search_runs()\n",
    "\n",
    "runs[[\n",
    "    \"tags.mlflow.runName\",\n",
    "    \"metrics.rmse\",\n",
    "    \"params.model_type\"\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f5d9167-1016-405d-8812-04a96e832624",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>rmse</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>6.877885e+07</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>7.987845e+07</td>\n",
       "      <td>DecisionTree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>1.957067e+07</td>\n",
       "      <td>LinearRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>1.957067e+07</td>\n",
       "      <td>LinearRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>1.957067e+07</td>\n",
       "      <td>LinearRegression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           run_name          rmse             model\n",
       "0      RandomForest  6.877885e+07      RandomForest\n",
       "1      DecisionTree  7.987845e+07      DecisionTree\n",
       "2  LinearRegression  1.957067e+07  LinearRegression\n",
       "3  LinearRegression  1.957067e+07  LinearRegression\n",
       "4  LinearRegression  1.957067e+07  LinearRegression"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_runs = runs[[\n",
    "    \"tags.mlflow.runName\",\n",
    "    \"metrics.rmse\",\n",
    "    \"params.model_type\"\n",
    "]].rename(columns={\n",
    "    \"tags.mlflow.runName\": \"run_name\",\n",
    "    \"metrics.rmse\": \"rmse\",\n",
    "    \"params.model_type\": \"model\"\n",
    "})\n",
    "\n",
    "clean_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aa9c96c-9401-4977-8224-0bc650672a03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Build Spark ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c0bd161-75ab-48da-a9c6-1cea92167aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/21 08:50:46 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/01/21 08:50:48 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-07530eca-bcfd-48b6-bf02-e9/tmpkj89l0yo/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/01/21 08:50:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline RMSE: 68778849.78263079\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Unity Catalog requirement\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/workspace/ecommerce/mlflow_tmp\"\n",
    "\n",
    "# Set MLflow Experiment\n",
    "mlflow.set_experiment(\"/day13-mlflow-pipeline\")\n",
    "\n",
    "# Load data from Unity Catalog\n",
    "data = (\n",
    "    spark.table(\"ecommerce.silver.daily_sales\")\n",
    "    .select(\"total_events\", \"total_revenue\")\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Train-test split\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Feature engineering\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"total_events\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Model\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"total_revenue\",\n",
    "    numTrees=50\n",
    ")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "with mlflow.start_run(run_name=\"RandomForest_Pipeline\"):\n",
    "\n",
    "    pipeline_model = pipeline.fit(train)\n",
    "    predictions = pipeline_model.transform(test)\n",
    "\n",
    "    evaluator = RegressionEvaluator(\n",
    "        labelCol=\"total_revenue\",\n",
    "        predictionCol=\"prediction\",\n",
    "        metricName=\"rmse\"\n",
    "    )\n",
    "\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "    # Log metadata\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestPipeline\")\n",
    "    mlflow.log_param(\"features\", \"total_events\")\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    # Log pipeline model (UC-safe)\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=pipeline_model,\n",
    "        artifact_path=\"pipeline_model\"\n",
    "    )\n",
    "\n",
    "    print(\"Pipeline RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412f1b50-f986-41b8-8bf8-a774d3bbdf1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6abfaf1e-397c-44f6-9dde-5bbf34e66265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: LinearRegression\nBest RMSE: 19570667.462254103\n"
     ]
    }
   ],
   "source": [
    "best_run = clean_runs.sort_values(\"rmse\").iloc[0]\n",
    "\n",
    "print(\"Best Model:\", best_run[\"model\"])\n",
    "print(\"Best RMSE:\", best_run[\"rmse\"])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_13_Model_Comparison_Feature_Engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}